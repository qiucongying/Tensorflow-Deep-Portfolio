{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import quandl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "top10_data = pd.read_csv(\"return_data_top10.csv\")\n",
    "\n",
    "# Equalize the data\n",
    "min_date = pd.to_datetime('2013-10-1')\n",
    "top10_data = top10_data[top10_data.Date >= '2013-10-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Date</th>\n",
       "      <th>2013-10-10</th>\n",
       "      <th>2013-10-11</th>\n",
       "      <th>2013-10-14</th>\n",
       "      <th>2013-10-15</th>\n",
       "      <th>2013-10-16</th>\n",
       "      <th>2013-10-17</th>\n",
       "      <th>2013-10-18</th>\n",
       "      <th>2013-10-21</th>\n",
       "      <th>2013-10-22</th>\n",
       "      <th>2013-10-23</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-09-15</th>\n",
       "      <th>2017-09-18</th>\n",
       "      <th>2017-09-19</th>\n",
       "      <th>2017-09-20</th>\n",
       "      <th>2017-09-21</th>\n",
       "      <th>2017-09-22</th>\n",
       "      <th>2017-09-25</th>\n",
       "      <th>2017-09-26</th>\n",
       "      <th>2017-09-27</th>\n",
       "      <th>2017-09-28</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>0.024508</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>0.009796</td>\n",
       "      <td>0.013239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>-0.008822</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>-0.006160</td>\n",
       "      <td>0.005480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRK_B</th>\n",
       "      <td>0.008447</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>-0.009192</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.008486</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>-0.003590</td>\n",
       "      <td>-0.007098</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FB</th>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.033030</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>-0.021820</td>\n",
       "      <td>-0.014713</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009150</td>\n",
       "      <td>0.014764</td>\n",
       "      <td>-0.002029</td>\n",
       "      <td>-0.006157</td>\n",
       "      <td>-0.003331</td>\n",
       "      <td>-0.044975</td>\n",
       "      <td>0.008227</td>\n",
       "      <td>0.021131</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>0.012327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GE</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.007793</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>0.035251</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.004591</td>\n",
       "      <td>-0.012298</td>\n",
       "      <td>0.009339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022148</td>\n",
       "      <td>-0.010630</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.005334</td>\n",
       "      <td>-0.002475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ</th>\n",
       "      <td>0.019025</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.013121</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.004693</td>\n",
       "      <td>0.012719</td>\n",
       "      <td>-0.002815</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>-0.014791</td>\n",
       "      <td>-0.011034</td>\n",
       "      <td>-0.002732</td>\n",
       "      <td>-0.001674</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>-0.009088</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>0.004171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.010960</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>-0.011718</td>\n",
       "      <td>-0.023713</td>\n",
       "      <td>-0.001185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001992</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>-0.006628</td>\n",
       "      <td>-0.009741</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>-0.015455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.008393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG</th>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.005541</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>0.006594</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.010950</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>-0.018644</td>\n",
       "      <td>-0.004318</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>-0.019212</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>0.001464</td>\n",
       "      <td>-0.008187</td>\n",
       "      <td>-0.006191</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>-0.018396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>-0.005464</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.013734</td>\n",
       "      <td>-0.010225</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>0.003330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WFC</th>\n",
       "      <td>-0.000241</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>-0.004192</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>-0.004055</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.016590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>0.010576</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>-0.009247</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.003655</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>-0.002555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Date    2013-10-10  2013-10-11  2013-10-14  2013-10-15  2013-10-16  \\\n",
       "Ticker                                                               \n",
       "AAPL      0.006482    0.006550    0.005322    0.004881    0.006757   \n",
       "BRK_B     0.008447    0.001641   -0.011810    0.017971    0.001971   \n",
       "FB        0.001223    0.008145   -0.000202    0.033030    0.021023   \n",
       "GE        0.006186   -0.000820   -0.007793    0.007028    0.013136   \n",
       "JNJ       0.019025    0.003913    0.001448    0.013121    0.009439   \n",
       "MSFT      0.010960    0.009376    0.001161    0.004349    0.008083   \n",
       "PG        0.007575    0.003313   -0.014478    0.017287    0.013786   \n",
       "T         0.001464   -0.008187   -0.006191    0.014536    0.007018   \n",
       "WFC      -0.000241    0.007724   -0.005030    0.016129    0.011135   \n",
       "XOM       0.010576    0.007476   -0.009247    0.005991    0.001947   \n",
       "\n",
       "Date    2013-10-17  2013-10-18  2013-10-21  2013-10-22  2013-10-23  \\\n",
       "Ticker                                                               \n",
       "AAPL      0.008702    0.024508   -0.002867    0.009796    0.013239   \n",
       "BRK_B     0.000428   -0.000684    0.005133   -0.009192    0.003694   \n",
       "FB        0.038498   -0.006824   -0.021820   -0.014713    0.010501   \n",
       "GE        0.035251    0.023092   -0.004591   -0.012298    0.009339   \n",
       "JNJ      -0.003697   -0.004693    0.012719   -0.002815    0.002714   \n",
       "MSFT      0.001145    0.000858   -0.011718   -0.023713   -0.001185   \n",
       "PG       -0.000126   -0.005541    0.017855    0.006594   -0.003708   \n",
       "T         0.004936    0.017625    0.000284    0.001391   -0.018396   \n",
       "WFC       0.000000   -0.001406    0.007508   -0.004192   -0.001637   \n",
       "XOM       0.000800   -0.003655    0.008598   -0.004206    0.009246   \n",
       "\n",
       "Date       ...      2017-09-15  2017-09-18  2017-09-19  2017-09-20  \\\n",
       "Ticker     ...                                                       \n",
       "AAPL       ...       -0.007568    0.000378   -0.016758   -0.017172   \n",
       "BRK_B      ...        0.008783    0.008486    0.004426   -0.003590   \n",
       "FB         ...       -0.009150    0.014764   -0.002029   -0.006157   \n",
       "GE         ...        0.022148   -0.010630    0.004959    0.017681   \n",
       "JNJ        ...        0.006917   -0.001182   -0.014791   -0.011034   \n",
       "MSFT       ...       -0.001992    0.003725   -0.006628   -0.009741   \n",
       "PG         ...       -0.001287    0.010950    0.002442   -0.018644   \n",
       "T          ...        0.008625    0.021112    0.005758   -0.005464   \n",
       "WFC        ...        0.020325    0.012332    0.007309    0.005767   \n",
       "XOM        ...        0.000250    0.001623    0.004114   -0.008194   \n",
       "\n",
       "Date    2017-09-21  2017-09-22  2017-09-25  2017-09-26  2017-09-27  2017-09-28  \n",
       "Ticker                                                                          \n",
       "AAPL     -0.009779   -0.008822    0.017204    0.007118   -0.006160    0.005480  \n",
       "BRK_B    -0.007098    0.003629   -0.003616    0.007753   -0.000327    0.000600  \n",
       "FB       -0.003331   -0.044975    0.008227    0.021131    0.006262    0.012327  \n",
       "GE        0.004848    0.009650   -0.007168   -0.022463   -0.005334   -0.002475  \n",
       "JNJ      -0.002732   -0.001674   -0.001753   -0.009088   -0.002158    0.004171  \n",
       "MSFT      0.002695   -0.015455    0.000000    0.008054    0.000271    0.008393  \n",
       "PG       -0.004318    0.005204   -0.000755   -0.019212    0.000220    0.000990  \n",
       "T         0.009681    0.013734   -0.010225    0.001291    0.006964    0.003330  \n",
       "WFC       0.003515   -0.004055   -0.004257    0.007621    0.000738    0.016590  \n",
       "XOM       0.000376    0.013263   -0.001111    0.006676    0.009333   -0.002555  \n",
       "\n",
       "[10 rows x 1000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.pivot_table(data=top10_data, columns=[\"Date\"], index=[\"Ticker\"], values=\"forward_return\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Dropout + LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/nmt#training--how-to-build-our-first-nmt-system\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 10000\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "hidden_dim = 4\n",
    "batch_size = 1\n",
    "source_sequence_length = 500\n",
    "num_tickers = 10\n",
    "_time_major = True \n",
    "Pin = 1.0\n",
    "Pout = 0.80\n",
    "Pstate = 0.80\n",
    "\"\"\"\n",
    " If true, these Tensors must be shaped [max_time, batch_size, depth]. \n",
    " If false, these Tensors must be shaped [batch_size, max_time, depth]. \n",
    " Using time_major = True is a bit more efficient\n",
    "\"\"\"\n",
    "\n",
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Graph Placeholders\n",
    "encoder_inputs = tf.placeholder(shape=(source_sequence_length, None, num_tickers), \n",
    "                                dtype=tf.float64, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(source_sequence_length, None, num_tickers), \n",
    "                                 dtype=tf.float64, name='decoder_targets')\n",
    "\n",
    "# Build Encoder cell\n",
    "encoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim,\n",
    "                                       forget_bias=1.0,\n",
    "                                       state_is_tuple=True,\n",
    "                                       activation=tf.tanh,\n",
    "                                       reuse=None)\n",
    "\n",
    "# Introduce Dropout to the Encoding\n",
    "encoder_dropout = tf.contrib.rnn.DropoutWrapper(encoder_cell,\n",
    "                                                input_keep_prob=Pin,\n",
    "                                                output_keep_prob=Pout,\n",
    "                                                state_keep_prob=Pstate)\n",
    "\n",
    "# Build Simple Decoder Cell\n",
    "decoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=num_tickers,\n",
    "                                       forget_bias=1.0,\n",
    "                                       state_is_tuple=True,\n",
    "                                       activation=tf.tanh,\n",
    "                                       reuse=None)\n",
    "\n",
    "# Compile Deep Network\n",
    "autoencoder_layers = tf.nn.rnn_cell.MultiRNNCell([encoder_dropout, decoder_cell], \n",
    "                                             state_is_tuple=True)\n",
    "\n",
    "\n",
    "# Run Dynamic RNN\n",
    "#   encoder_outpus: [batch_size, max_time, num_units]\n",
    "#   encoder_state: [batch_size, num_units]\n",
    "outputs, state = tf.nn.dynamic_rnn(autoencoder_layers,\n",
    "                                   encoder_inputs,\n",
    "                                   sequence_length=[source_sequence_length],\n",
    "                                   time_major=_time_major,\n",
    "                                   dtype=tf.float64)\n",
    "\n",
    "# Loss Metric (unregularized)\n",
    "loss = tf.losses.mean_squared_error(labels=decoder_targets, predictions=outputs)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Optimizer\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training step: 0 - Loss = [0.00016152348]\n",
      "predition:  [[[  9.18491965e-05  -3.22693197e-04   3.86958118e-05 ...,  -2.41205594e-04\n",
      "    -9.12903771e-05  -3.15538821e-04]]\n",
      "\n",
      " [[ -2.69229401e-05  -3.19823338e-04   2.02388515e-05 ...,  -3.14717809e-04\n",
      "    -1.58045025e-04  -3.40541110e-04]]\n",
      "\n",
      " [[ -8.67193230e-05  -3.60489515e-04   1.61020274e-06 ...,  -3.05083508e-04\n",
      "    -1.92323016e-04  -2.66470260e-04]]\n",
      "\n",
      " ..., \n",
      " [[  3.75384757e-04   5.85141547e-04  -7.98567642e-04 ...,   9.38545918e-04\n",
      "     9.32887299e-04   1.42529013e-03]]\n",
      "\n",
      " [[  7.11640266e-04  -8.82901732e-04  -6.55941040e-04 ...,   5.82676856e-04\n",
      "     7.50226933e-04   9.81898588e-04]]\n",
      "\n",
      " [[  3.57023872e-04  -4.51177938e-04  -3.25748920e-04 ...,   4.95364963e-04\n",
      "     7.56236106e-04   9.29943704e-04]]]\n",
      "--------------------------------------------------\n",
      "Training step: 50 - Loss = [0.00016099861]\n",
      "--------------------------------------------------\n",
      "Training step: 100 - Loss = [0.00016038489]\n",
      "predition:  [[[  4.99788099e-04  -3.64108622e-04   6.31284152e-04 ...,  -1.30674966e-04\n",
      "     5.41946881e-05  -1.81149571e-04]]\n",
      "\n",
      " [[  7.60058006e-04  -4.97345410e-04   1.05154052e-03 ...,  -2.07507933e-04\n",
      "     1.18196114e-04  -3.01970114e-04]]\n",
      "\n",
      " [[  8.59462515e-04  -3.02083398e-04   1.31885546e-03 ...,  -5.31461745e-05\n",
      "     2.52280549e-04  -1.35561458e-04]]\n",
      "\n",
      " ..., \n",
      " [[  9.89953584e-04   8.14856799e-04   1.67547701e-03 ...,  -2.00893019e-04\n",
      "     1.40985380e-03  -9.86287540e-04]]\n",
      "\n",
      " [[  1.02631675e-03   5.59398457e-04   1.72470017e-03 ...,  -2.35190225e-04\n",
      "     1.39913443e-03  -1.09648964e-03]]\n",
      "\n",
      " [[  8.24159346e-04   1.03706374e-03   1.87699266e-03 ...,   9.12198273e-05\n",
      "     1.78134833e-03  -8.03268809e-04]]]\n",
      "--------------------------------------------------\n",
      "Training step: 150 - Loss = [0.00016067164]\n",
      "--------------------------------------------------\n",
      "Training step: 200 - Loss = [0.00016049926]\n",
      "predition:  [[[  5.19751641e-04  -3.87841723e-04   6.01037967e-04 ...,  -8.02028668e-05\n",
      "     1.65499387e-05  -1.16715985e-04]]\n",
      "\n",
      " [[  6.91536885e-04  -4.10574411e-04   9.33273938e-04 ...,  -4.27382280e-05\n",
      "     7.01910660e-05  -3.76955301e-05]]\n",
      "\n",
      " [[  8.88129860e-04  -5.24817161e-04   1.19883528e-03 ...,  -8.84079692e-05\n",
      "     1.35179019e-04  -1.28821960e-04]]\n",
      "\n",
      " ..., \n",
      " [[  1.65575447e-03  -1.16731610e-03   1.79805048e-03 ...,   1.39025038e-04\n",
      "     1.32491573e-03  -3.06457517e-04]]\n",
      "\n",
      " [[  1.95627662e-03  -1.99211900e-03   1.93022919e-03 ...,   2.62808879e-04\n",
      "     1.33748101e-03  -1.47017132e-04]]\n",
      "\n",
      " [[  1.95792254e-03  -2.10705652e-03   2.28942303e-03 ...,   4.07348389e-04\n",
      "     1.78262124e-03  -7.44195831e-05]]]\n",
      "--------------------------------------------------\n",
      "Training step: 250 - Loss = [0.00016026676]\n",
      "--------------------------------------------------\n",
      "Training step: 300 - Loss = [0.00016006618]\n",
      "predition:  [[[  5.44264353e-04  -2.84275382e-04   6.38424586e-04 ...,   6.94932907e-06\n",
      "     5.12049208e-05  -3.61188066e-05]]\n",
      "\n",
      " [[  7.95997978e-04  -4.26264342e-04   9.85015612e-04 ...,  -4.49206932e-05\n",
      "     7.37652578e-05  -1.04897593e-04]]\n",
      "\n",
      " [[  1.00573201e-03  -4.49658491e-04   1.27437027e-03 ...,   3.12977083e-05\n",
      "     1.79007537e-04  -8.36192708e-05]]\n",
      "\n",
      " ..., \n",
      " [[  7.75801768e-04  -9.78898090e-04   1.02244462e-03 ...,  -7.18787803e-04\n",
      "     1.70878491e-05  -5.45328418e-04]]\n",
      "\n",
      " [[  9.46107033e-04  -1.81399035e-03   1.09788982e-03 ...,  -7.13445713e-04\n",
      "    -3.07559870e-05  -4.66237276e-04]]\n",
      "\n",
      " [[  9.33307189e-04  -1.88634634e-03   1.44187555e-03 ...,  -4.58547994e-04\n",
      "     3.57540492e-04  -2.32981864e-04]]]\n",
      "--------------------------------------------------\n",
      "Training step: 350 - Loss = [0.00015989527]\n",
      "--------------------------------------------------\n",
      "Training step: 400 - Loss = [0.00015957096]\n",
      "predition:  [[[  3.68664970e-04  -8.98695038e-05   5.71650705e-04 ...,   9.18915637e-06\n",
      "     8.81796752e-05   2.68012657e-05]]\n",
      "\n",
      " [[  6.74814514e-04  -2.12243592e-04   9.84962168e-04 ...,  -2.05478554e-05\n",
      "     2.25695102e-04  -8.25915340e-05]]\n",
      "\n",
      " [[  9.12138221e-04  -2.06234930e-04   1.31415659e-03 ...,   6.56957816e-05\n",
      "     4.12231119e-04  -9.35263785e-05]]\n",
      "\n",
      " ..., \n",
      " [[  1.04620363e-03  -4.68930196e-04   1.29224035e-03 ...,  -5.31873393e-04\n",
      "     6.80366455e-05  -7.42174679e-04]]\n",
      "\n",
      " [[  1.08498234e-03  -1.67662135e-04   1.54263373e-03 ...,  -1.87435600e-04\n",
      "     3.13572407e-04  -5.74509707e-04]]\n",
      "\n",
      " [[  1.09018859e-03   2.77971979e-05   1.60703611e-03 ...,  -2.10068817e-05\n",
      "     4.25929877e-04  -4.59482867e-04]]]\n",
      "--------------------------------------------------\n",
      "Training step: 450 - Loss = [0.00015888053]\n",
      "==================================================\n",
      "Overall Training Time: 746.2298016548157\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "\n",
    "\"\"\"\n",
    "The loss does not change significantly, so I reduce the # of step into 500.\n",
    "\"\"\"\n",
    "num_steps = 500\n",
    "x_train = df.values[0:, :500].T.reshape((500, 1, 10))\n",
    "y_train = df.values[0:, :500].T.reshape((500, 1, 10))\n",
    "\n",
    "tic = time.time()        # Initialize time count\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        _, train_loss, pred = sess.run([optimizer, loss, outputs], feed_dict={encoder_inputs:x_train, \n",
    "                                                               decoder_targets:y_train})\n",
    "\n",
    "        if (step % 50 == 0):\n",
    "            val_loss = sess.run([loss], feed_dict={encoder_inputs:x_train, \n",
    "                                                     decoder_targets:y_train})\n",
    "            \n",
    "            print(\"-\"*50)\n",
    "            print(\"Training step: {0} - Loss = {1}\".format(step, val_loss))\n",
    "        if (step % 100 == 0):\n",
    "#             pred = sess.run([outputs], feed_dict={encoder_inputs:x_train})\n",
    "            print(\"predition: \", pred)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "toc = time.time()\n",
    "print(\"Overall Training Time: {}\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Result for LSTM + Dropout + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:  [[[ 0.00648234  0.00844653  0.00122324 ...,  0.00146413 -0.00024131\n",
      "    0.01057648]]\n",
      "\n",
      " [[ 0.00655017  0.00164062  0.00814498 ..., -0.00818713  0.00772387\n",
      "    0.00747556]]\n",
      "\n",
      " [[ 0.00532215 -0.01181034 -0.00020198 ..., -0.00619104 -0.00502994\n",
      "   -0.00924658]]\n",
      "\n",
      " ..., \n",
      " [[ 0.0073006  -0.00038497  0.01231446 ...,  0.00338149 -0.00349922\n",
      "    0.02457467]]\n",
      "\n",
      " [[ 0.00362384  0.01964107  0.02107092 ...,  0.02420343  0.02243465\n",
      "    0.01225619]]\n",
      "\n",
      " [[ 0.00478426 -0.00898927 -0.01287097 ..., -0.00358959 -0.00725052\n",
      "    0.01406067]]]\n",
      "predition:  [[[  5.38497022e-04  -3.37411967e-04   5.78576653e-04 ...,  -2.84398388e-05\n",
      "     1.87144766e-05  -5.41934955e-05]]\n",
      "\n",
      " [[  8.45671394e-04  -3.78922022e-04   9.92880403e-04 ...,   4.34507207e-05\n",
      "     1.14373163e-04  -4.67834116e-05]]\n",
      "\n",
      " [[  1.00146152e-03  -4.04677504e-04   1.23563857e-03 ...,   6.80875388e-05\n",
      "     2.33132487e-04  -7.69692333e-05]]\n",
      "\n",
      " ..., \n",
      " [[  9.78645248e-04   8.58420235e-04   1.52050034e-03 ...,   3.48766233e-04\n",
      "     1.31245440e-03  -5.37348358e-05]]\n",
      "\n",
      " [[  1.06819431e-03   4.46967139e-04   1.50926917e-03 ...,   5.03117518e-04\n",
      "     1.42524569e-03   2.01423460e-04]]\n",
      "\n",
      " [[  9.97864187e-04   5.30236668e-04   1.67720656e-03 ...,   7.58874749e-04\n",
      "     1.79759775e-03   5.08701950e-04]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data: \", y_train)\n",
    "print(\"predition: \", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single LSTM Layer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, ?, 10)\n",
      "(500, ?, 10)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 10000\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "hidden_dim = 10\n",
    "batch_size = 1\n",
    "source_sequence_length = 500\n",
    "num_tickers = 10\n",
    "_time_major = True \n",
    "\n",
    "\"\"\"\n",
    " If true, these Tensors must be shaped [max_time, batch_size, depth]. \n",
    " If false, these Tensors must be shaped [batch_size, max_time, depth]. \n",
    " Using time_major = True is a bit more efficient\n",
    " \n",
    " To make the shape consistent, change the hidden_dim from 4 to 10. Then we get the shape (500, ?, 10)\n",
    "\"\"\"\n",
    "\n",
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Graph Placeholders\n",
    "encoder_inputs = tf.placeholder(shape=(source_sequence_length, None, num_tickers), \n",
    "                                dtype=tf.float64, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(source_sequence_length, None, num_tickers), \n",
    "                                 dtype=tf.float64, name='decoder_targets')\n",
    "\n",
    "# Build Encoder cell\n",
    "encoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim,\n",
    "                                       forget_bias=1.0,\n",
    "                                       state_is_tuple=True,\n",
    "                                       activation=tf.tanh,\n",
    "                                       reuse=None)\n",
    "\n",
    "outputs, _ = tf.nn.dynamic_rnn(encoder_cell,\n",
    "                               encoder_inputs,\n",
    "                               sequence_length=[source_sequence_length],\n",
    "                               time_major=_time_major,\n",
    "                               dtype=tf.float64)\n",
    "print(decoder_targets.shape)\n",
    "print(outputs.shape)\n",
    "\n",
    "# Loss Metric (unregularized)\n",
    "loss = tf.losses.mean_squared_error(labels=decoder_targets, predictions=outputs)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Optimizer\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training step: 0 - Loss = [0.00016431577]\n",
      "predition:  [[[  5.88245232e-05  -3.26075306e-04  -1.11841901e-03 ...,   2.78347123e-04\n",
      "    -7.67959855e-04  -7.81038773e-04]]\n",
      "\n",
      " [[  4.59992908e-04  -5.77262734e-04  -1.22846708e-03 ...,   5.35995465e-04\n",
      "    -1.32365505e-03  -8.51807622e-05]]\n",
      "\n",
      " [[ -1.04854913e-04  -3.09512773e-04  -2.11572458e-03 ...,   5.82904241e-04\n",
      "     1.02186393e-03  -1.63457995e-03]]\n",
      "\n",
      " ..., \n",
      " [[  3.61842949e-03  -2.98828628e-04  -4.47722709e-03 ...,   1.12466364e-03\n",
      "    -3.65029169e-03   3.99727986e-03]]\n",
      "\n",
      " [[  8.64124684e-03  -2.67470949e-03  -3.61403043e-03 ...,  -3.26807857e-03\n",
      "    -8.42490950e-03   6.14149939e-03]]\n",
      "\n",
      " [[  7.13410830e-03  -1.16735181e-03  -4.39539431e-03 ...,  -2.87129104e-04\n",
      "    -7.68975293e-03   5.63001273e-03]]]\n",
      "--------------------------------------------------\n",
      "Training step: 50 - Loss = [0.00016228556]\n",
      "--------------------------------------------------\n",
      "Training step: 100 - Loss = [0.00016063209]\n",
      "predition:  [[[  2.76129787e-04   4.22790467e-05  -5.86418004e-04 ...,   5.58502789e-04\n",
      "    -3.27145488e-04  -6.94069635e-04]]\n",
      "\n",
      " [[  8.17644675e-04  -4.16066243e-05  -3.73396000e-04 ...,   9.53581328e-04\n",
      "    -6.45252716e-04  -9.36995456e-06]]\n",
      "\n",
      " [[  1.54795517e-04   1.34377979e-04  -1.20797511e-03 ...,   8.94898536e-04\n",
      "     1.62571837e-03  -1.80090646e-03]]\n",
      "\n",
      " ..., \n",
      " [[  4.34633281e-03   6.07433551e-04  -2.62878890e-03 ...,   1.78919268e-03\n",
      "    -2.57982536e-03   4.01460338e-03]]\n",
      "\n",
      " [[  9.64850219e-03  -1.40426215e-03  -1.39873966e-03 ...,  -2.17414686e-03\n",
      "    -6.91612948e-03   6.53888630e-03]]\n",
      "\n",
      " [[  7.92080348e-03  -1.81506299e-04  -2.44881521e-03 ...,   5.58391844e-04\n",
      "    -6.36795007e-03   5.79239096e-03]]]\n",
      "--------------------------------------------------\n",
      "Training step: 150 - Loss = [0.00015903285]\n",
      "--------------------------------------------------\n",
      "Training step: 200 - Loss = [0.00015748378]\n",
      "predition:  [[[  4.31010378e-04   1.95822240e-04  -4.12204251e-04 ...,   7.07742991e-04\n",
      "    -1.61428910e-04  -5.40081457e-04]]\n",
      "\n",
      " [[  1.05980876e-03   1.34404545e-04  -1.54940655e-04 ...,   1.14078558e-03\n",
      "    -4.30976566e-04   1.90834463e-04]]\n",
      "\n",
      " [[  2.38838588e-04   1.24695339e-04  -1.15178650e-03 ...,   9.12778667e-04\n",
      "     1.64591290e-03  -1.80065829e-03]]\n",
      "\n",
      " ..., \n",
      " [[  4.61619188e-03   8.02898436e-04  -2.28856873e-03 ...,   1.96949132e-03\n",
      "    -2.22339230e-03   4.28410844e-03]]\n",
      "\n",
      " [[  1.02386025e-02  -8.63075991e-04  -7.16503659e-04 ...,  -1.59237240e-03\n",
      "    -6.15140667e-03   7.18585442e-03]]\n",
      "\n",
      " [[  8.26802584e-03   7.59574113e-05  -2.04963606e-03 ...,   8.84336232e-04\n",
      "    -5.81500862e-03   6.19974454e-03]]]\n",
      "--------------------------------------------------\n",
      "Training step: 250 - Loss = [0.00015597977]\n",
      "--------------------------------------------------\n",
      "Training step: 300 - Loss = [0.00015451499]\n",
      "predition:  [[[  5.86180567e-04   3.43561644e-04  -2.44131043e-04 ...,   8.51837313e-04\n",
      "    -8.46591333e-08  -3.82932910e-04]]\n",
      "\n",
      " [[  1.30287419e-03   3.04596422e-04   5.38203197e-05 ...,   1.31754602e-03\n",
      "    -2.23522817e-04   3.93023529e-04]]\n",
      "\n",
      " [[  3.23297755e-04   1.17174557e-04  -1.10510585e-03 ...,   9.23513852e-04\n",
      "     1.66624528e-03  -1.80174050e-03]]\n",
      "\n",
      " ..., \n",
      " [[  4.88304570e-03   9.93508663e-04  -1.95911105e-03 ...,   2.15194800e-03\n",
      "    -1.87024536e-03   4.55757040e-03]]\n",
      "\n",
      " [[  1.08262355e-02  -3.46489992e-04  -5.88459932e-05 ...,  -1.03709328e-03\n",
      "    -5.41829121e-03   7.82748676e-03]]\n",
      "\n",
      " [[  8.60252366e-03   3.08293557e-04  -1.68462244e-03 ...,   1.17835022e-03\n",
      "    -5.30572093e-03   6.59378159e-03]]]\n",
      "--------------------------------------------------\n",
      "Training step: 350 - Loss = [0.00015308363]\n",
      "--------------------------------------------------\n",
      "Training step: 400 - Loss = [0.00015168014]\n",
      "predition:  [[[  7.41275994e-04   4.86725297e-04  -8.03522006e-05 ...,   9.91092982e-04\n",
      "     1.56426981e-04  -2.22192820e-04]]\n",
      "\n",
      " [[  1.54352062e-03   4.68961956e-04   2.53921945e-04 ...,   1.48438472e-03\n",
      "    -2.46220553e-05   5.96221969e-04]]\n",
      "\n",
      " [[  4.03830813e-04   1.09383551e-04  -1.07251374e-03 ...,   9.26809154e-04\n",
      "     1.68317417e-03  -1.80634477e-03]]\n",
      "\n",
      " ..., \n",
      " [[  5.14430303e-03   1.17575196e-03  -1.64416978e-03 ...,   2.33438811e-03\n",
      "    -1.52939597e-03   4.83311813e-03]]\n",
      "\n",
      " [[  1.14010134e-02   1.42929643e-04   5.76742424e-04 ...,  -5.06647652e-04\n",
      "    -4.72008814e-03   8.46072920e-03]]\n",
      "\n",
      " [[  8.90889793e-03   5.05747872e-04  -1.36064059e-03 ...,   1.44331450e-03\n",
      "    -4.83970778e-03   6.96194325e-03]]]\n",
      "--------------------------------------------------\n",
      "Training step: 450 - Loss = [0.00015029941]\n",
      "==================================================\n",
      "Overall Training Time: 345.9702353477478\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "num_steps = 500\n",
    "x_train = df.values[0:, :500].T.reshape((500, 1, 10))\n",
    "y_train = df.values[0:, :500].T.reshape((500, 1, 10))\n",
    "\n",
    "tic = time.time()        # Initialize time count\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        _, train_loss, pred = sess.run([optimizer, loss, outputs], feed_dict={encoder_inputs:x_train, \n",
    "                                                               decoder_targets:y_train})\n",
    "\n",
    "        if (step % 50 == 0):\n",
    "            val_loss = sess.run([loss], feed_dict={encoder_inputs:x_train, \n",
    "                                                     decoder_targets:y_train})\n",
    "            \n",
    "            print(\"-\"*50)\n",
    "            print(\"Training step: {0} - Loss = {1}\".format(step, val_loss))\n",
    "        if (step % 100 == 0):\n",
    "#             pred = sess.run([outputs], feed_dict={encoder_inputs:x_train})\n",
    "            print(\"predition: \", pred)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "toc = time.time()\n",
    "print(\"Overall Training Time: {}\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Result for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Original data: \", y_train)\n",
    "print(\"predition: \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_encoder_net(encoder_input, decoder_target, keep_prob_in, keep_prob_out, keep_prob_state,\n",
    "                     hidden_dim, num_tickers, source_sequence_length, l2_norm=0.01, seed=235):\n",
    "    \n",
    "    # Build Encoder cell\n",
    "    encoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim,\n",
    "                                           forget_bias=1.0,\n",
    "                                           state_is_tuple=True,\n",
    "                                           activation=tf.tanh,\n",
    "                                           reuse=None)\n",
    "    \n",
    "    # Introduce Dropout to the Encoding\n",
    "    encoder_dropout = tf.contrib.rnn.DropoutWrapper(encoder_cell,\n",
    "                                                    input_keep_prob=keep_prob_in,\n",
    "                                                    output_keep_prob=keep_prob_out,\n",
    "                                                    state_keep_prob=keep_prob_state)\n",
    "    \n",
    "    \n",
    "    # Build Simple Decoder Cell\n",
    "    decoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=num_tickers,\n",
    "                                           forget_bias=1.0,\n",
    "                                           state_is_tuple=True,\n",
    "                                           activation=tf.tanh,\n",
    "                                           reuse=None)\n",
    "\n",
    "    # Compile Deep Network\n",
    "    autoencoder_layers = tf.nn.rnn_cell.MultiRNNCell([encoder_dropout, decoder_cell], \n",
    "                                                 state_is_tuple=True)\n",
    "\n",
    "\n",
    "    # Run Dynamic RNN\n",
    "    #   encoder_outpus: [batch_size, max_time, num_units]\n",
    "    #   encoder_state: [batch_size, num_units]\n",
    "    outputs, state = tf.nn.dynamic_rnn(autoencoder_layers,\n",
    "                                       encoder_inputs,\n",
    "                                       sequence_length=[source_sequence_length],\n",
    "                                       time_major=True,\n",
    "                                       dtype=tf.float64)\n",
    "\n",
    "    #fc_w = [fc_layer_0.weight, fc_layer_1.weight, fc_layer_2.weight]\n",
    "\n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #l2_loss = tf.reduce_sum([tf.norm(w) for w in fc_w])\n",
    "        \n",
    "        #label = tf.one_hot(input_y, output_size)\n",
    "        #cross_entropy_loss = tf.reduce_mean(\n",
    "        #    tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=fc_layer_2.output()),\n",
    "        #    name='cross_entropy')\n",
    "        #loss = tf.add(cross_entropy_loss, l2_norm * l2_loss, name='loss')\n",
    "        loss = tf.losses.mean_squared_error(labels=decoder_targets, predictions=outputs)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        tf.summary.scalar('auto_encode_loss', loss)\n",
    "\n",
    "    return fc_layer_2.output(), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(loss, lr=1e-3):\n",
    "    with tf.name_scope('train_step'):\n",
    "        step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(loss)\n",
    "    return step\n",
    "\n",
    "def auto_encoder_training(X_train, y_train, X_val, y_val, keep_prob=0.7,\n",
    "                l2_norm=0.01, seed=235, learning_rate=1e-2, epoch=20,\n",
    "                batch_size=245, verbose=True):\n",
    "    print(\"Building my Auto-Encoder Parameters: \")\n",
    "    print(\"l2_norm={}\".format(l2_norm))\n",
    "    print(\"seed={}\".format(seed))\n",
    "    print(\"learning_rate={}\".format(learning_rate))\n",
    "\n",
    "    # define the variables and parameter needed during training\n",
    "    with tf.name_scope('inputs'):\n",
    "        xs = tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)\n",
    "        ys = tf.placeholder(shape=[None, ], dtype=tf.int64)\n",
    "        ss = tf.placeholder(shape=[None, 5], dtype=tf.float32)\n",
    "        kp = tf.placeholder(dtype=tf.float32)\n",
    "     \n",
    "    output, loss = dn2387_NN(xs, ys, kp)\n",
    "    predictions = predict(ss)\n",
    "    iters = int(X_train.shape[0] / batch_size)\n",
    "    print('number of batches for training: {}'.format(iters))\n",
    "\n",
    "    step = train_step(loss)\n",
    "    eve = evaluate(output, ys)\n",
    "\n",
    "    iter_total = 0\n",
    "    best_acc = 0\n",
    "    cur_model_name = 'dn2387_{}'.format(int(time.time()))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        merge = tf.summary.merge_all()\n",
    "\n",
    "        writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "                              \n",
    "        for epc in range(epoch):\n",
    "            print(\"epoch {} \".format(epc + 1))\n",
    "\n",
    "            for itr in range(iters):\n",
    "                iter_total += 1\n",
    "\n",
    "                training_batch_x = X_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "                training_batch_y = y_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "                \n",
    "                #import pdb\n",
    "                #pdb.set_trace()\n",
    "                _, cur_loss = sess.run([step, loss], feed_dict={xs: training_batch_x, ys: training_batch_y, kp: keep_prob})\n",
    "\n",
    "                if iter_total % 10 == 0:\n",
    "                    # do validation\n",
    "                    valid_eve, merge_result = sess.run([eve, merge], feed_dict={xs: X_val, ys: y_val, kp:1.0})\n",
    "                    valid_acc = 100 - valid_eve * 100 / y_val.shape[0]\n",
    "                    if verbose:\n",
    "                        print('{}/{} loss: {} validation accuracy : {}%'.format(\n",
    "                            batch_size * (itr + 1),\n",
    "                            X_train.shape[0],\n",
    "                            cur_loss,\n",
    "                            valid_acc))\n",
    "\n",
    "                    # save the merge result summary\n",
    "                    writer.add_summary(merge_result, iter_total)\n",
    "\n",
    "                    # when achieve the best validation accuracy, we store the model paramters\n",
    "                    if valid_acc > best_acc:\n",
    "                        print('Best validation accuracy! iteration:{} accuracy: {}%'.format(iter_total, valid_acc))\n",
    "                        best_acc = valid_acc\n",
    "                        saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "\n",
    "    print(\"Training ends. The best valid accuracy is {}. Model named {}.\".format(best_acc, cur_model_name))\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
