{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import quandl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "top10_data = pd.read_csv(\"return_data_top10.csv\")\n",
    "\n",
    "# Equalize the data\n",
    "min_date = pd.to_datetime('2013-10-1')\n",
    "top10_data = top10_data[top10_data.Date >= '2013-10-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Date</th>\n",
       "      <th>2013-10-10</th>\n",
       "      <th>2013-10-11</th>\n",
       "      <th>2013-10-14</th>\n",
       "      <th>2013-10-15</th>\n",
       "      <th>2013-10-16</th>\n",
       "      <th>2013-10-17</th>\n",
       "      <th>2013-10-18</th>\n",
       "      <th>2013-10-21</th>\n",
       "      <th>2013-10-22</th>\n",
       "      <th>2013-10-23</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-09-15</th>\n",
       "      <th>2017-09-18</th>\n",
       "      <th>2017-09-19</th>\n",
       "      <th>2017-09-20</th>\n",
       "      <th>2017-09-21</th>\n",
       "      <th>2017-09-22</th>\n",
       "      <th>2017-09-25</th>\n",
       "      <th>2017-09-26</th>\n",
       "      <th>2017-09-27</th>\n",
       "      <th>2017-09-28</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>0.024508</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>0.009796</td>\n",
       "      <td>0.013239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>-0.008822</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>-0.006160</td>\n",
       "      <td>0.005480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRK_B</th>\n",
       "      <td>0.008447</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>-0.009192</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.008486</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>-0.003590</td>\n",
       "      <td>-0.007098</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FB</th>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.033030</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>-0.021820</td>\n",
       "      <td>-0.014713</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009150</td>\n",
       "      <td>0.014764</td>\n",
       "      <td>-0.002029</td>\n",
       "      <td>-0.006157</td>\n",
       "      <td>-0.003331</td>\n",
       "      <td>-0.044975</td>\n",
       "      <td>0.008227</td>\n",
       "      <td>0.021131</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>0.012327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GE</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.007793</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>0.035251</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.004591</td>\n",
       "      <td>-0.012298</td>\n",
       "      <td>0.009339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022148</td>\n",
       "      <td>-0.010630</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.005334</td>\n",
       "      <td>-0.002475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ</th>\n",
       "      <td>0.019025</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.013121</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.004693</td>\n",
       "      <td>0.012719</td>\n",
       "      <td>-0.002815</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>-0.014791</td>\n",
       "      <td>-0.011034</td>\n",
       "      <td>-0.002732</td>\n",
       "      <td>-0.001674</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>-0.009088</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>0.004171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.010960</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>-0.011718</td>\n",
       "      <td>-0.023713</td>\n",
       "      <td>-0.001185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001992</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>-0.006628</td>\n",
       "      <td>-0.009741</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>-0.015455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.008393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG</th>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.005541</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>0.006594</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.010950</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>-0.018644</td>\n",
       "      <td>-0.004318</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>-0.019212</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>0.001464</td>\n",
       "      <td>-0.008187</td>\n",
       "      <td>-0.006191</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>-0.018396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>-0.005464</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.013734</td>\n",
       "      <td>-0.010225</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>0.003330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WFC</th>\n",
       "      <td>-0.000241</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>-0.004192</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>-0.004055</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.016590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>0.010576</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>-0.009247</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.003655</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>-0.002555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Date    2013-10-10  2013-10-11  2013-10-14  2013-10-15  2013-10-16  \\\n",
       "Ticker                                                               \n",
       "AAPL      0.006482    0.006550    0.005322    0.004881    0.006757   \n",
       "BRK_B     0.008447    0.001641   -0.011810    0.017971    0.001971   \n",
       "FB        0.001223    0.008145   -0.000202    0.033030    0.021023   \n",
       "GE        0.006186   -0.000820   -0.007793    0.007028    0.013136   \n",
       "JNJ       0.019025    0.003913    0.001448    0.013121    0.009439   \n",
       "MSFT      0.010960    0.009376    0.001161    0.004349    0.008083   \n",
       "PG        0.007575    0.003313   -0.014478    0.017287    0.013786   \n",
       "T         0.001464   -0.008187   -0.006191    0.014536    0.007018   \n",
       "WFC      -0.000241    0.007724   -0.005030    0.016129    0.011135   \n",
       "XOM       0.010576    0.007476   -0.009247    0.005991    0.001947   \n",
       "\n",
       "Date    2013-10-17  2013-10-18  2013-10-21  2013-10-22  2013-10-23  \\\n",
       "Ticker                                                               \n",
       "AAPL      0.008702    0.024508   -0.002867    0.009796    0.013239   \n",
       "BRK_B     0.000428   -0.000684    0.005133   -0.009192    0.003694   \n",
       "FB        0.038498   -0.006824   -0.021820   -0.014713    0.010501   \n",
       "GE        0.035251    0.023092   -0.004591   -0.012298    0.009339   \n",
       "JNJ      -0.003697   -0.004693    0.012719   -0.002815    0.002714   \n",
       "MSFT      0.001145    0.000858   -0.011718   -0.023713   -0.001185   \n",
       "PG       -0.000126   -0.005541    0.017855    0.006594   -0.003708   \n",
       "T         0.004936    0.017625    0.000284    0.001391   -0.018396   \n",
       "WFC       0.000000   -0.001406    0.007508   -0.004192   -0.001637   \n",
       "XOM       0.000800   -0.003655    0.008598   -0.004206    0.009246   \n",
       "\n",
       "Date       ...      2017-09-15  2017-09-18  2017-09-19  2017-09-20  \\\n",
       "Ticker     ...                                                       \n",
       "AAPL       ...       -0.007568    0.000378   -0.016758   -0.017172   \n",
       "BRK_B      ...        0.008783    0.008486    0.004426   -0.003590   \n",
       "FB         ...       -0.009150    0.014764   -0.002029   -0.006157   \n",
       "GE         ...        0.022148   -0.010630    0.004959    0.017681   \n",
       "JNJ        ...        0.006917   -0.001182   -0.014791   -0.011034   \n",
       "MSFT       ...       -0.001992    0.003725   -0.006628   -0.009741   \n",
       "PG         ...       -0.001287    0.010950    0.002442   -0.018644   \n",
       "T          ...        0.008625    0.021112    0.005758   -0.005464   \n",
       "WFC        ...        0.020325    0.012332    0.007309    0.005767   \n",
       "XOM        ...        0.000250    0.001623    0.004114   -0.008194   \n",
       "\n",
       "Date    2017-09-21  2017-09-22  2017-09-25  2017-09-26  2017-09-27  2017-09-28  \n",
       "Ticker                                                                          \n",
       "AAPL     -0.009779   -0.008822    0.017204    0.007118   -0.006160    0.005480  \n",
       "BRK_B    -0.007098    0.003629   -0.003616    0.007753   -0.000327    0.000600  \n",
       "FB       -0.003331   -0.044975    0.008227    0.021131    0.006262    0.012327  \n",
       "GE        0.004848    0.009650   -0.007168   -0.022463   -0.005334   -0.002475  \n",
       "JNJ      -0.002732   -0.001674   -0.001753   -0.009088   -0.002158    0.004171  \n",
       "MSFT      0.002695   -0.015455    0.000000    0.008054    0.000271    0.008393  \n",
       "PG       -0.004318    0.005204   -0.000755   -0.019212    0.000220    0.000990  \n",
       "T         0.009681    0.013734   -0.010225    0.001291    0.006964    0.003330  \n",
       "WFC       0.003515   -0.004055   -0.004257    0.007621    0.000738    0.016590  \n",
       "XOM       0.000376    0.013263   -0.001111    0.006676    0.009333   -0.002555  \n",
       "\n",
       "[10 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.pivot_table(data=top10_data, columns=[\"Date\"], index=[\"Ticker\"], values=\"forward_return\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/nmt#training--how-to-build-our-first-nmt-system\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 10000\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "hidden_dim = 4\n",
    "batch_size = 1\n",
    "source_sequence_length = 500\n",
    "num_tickers = 10\n",
    "_time_major = True \n",
    "Pin = 1.0\n",
    "Pout = 0.80\n",
    "Pstate = 0.80\n",
    "\"\"\"\n",
    " If true, these Tensors must be shaped [max_time, batch_size, depth]. \n",
    " If false, these Tensors must be shaped [batch_size, max_time, depth]. \n",
    " Using time_major = True is a bit more efficient\n",
    "\"\"\"\n",
    "\n",
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Graph Placeholders\n",
    "encoder_inputs = tf.placeholder(shape=(source_sequence_length, None, num_tickers), \n",
    "                                dtype=tf.float64, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(source_sequence_length, None, num_tickers), \n",
    "                                 dtype=tf.float64, name='decoder_targets')\n",
    "\n",
    "# Build Encoder cell\n",
    "encoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim,\n",
    "                                       forget_bias=1.0,\n",
    "                                       state_is_tuple=True,\n",
    "                                       activation=tf.tanh,\n",
    "                                       reuse=None)\n",
    "\n",
    "# Introduce Dropout to the Encoding\n",
    "encoder_dropout = tf.contrib.rnn.DropoutWrapper(encoder_cell,\n",
    "                                                input_keep_prob=Pin,\n",
    "                                                output_keep_prob=Pout,\n",
    "                                                state_keep_prob=Pstate)\n",
    "\n",
    "# Build Simple Decoder Cell\n",
    "decoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=num_tickers,\n",
    "                                       forget_bias=1.0,\n",
    "                                       state_is_tuple=True,\n",
    "                                       activation=tf.tanh,\n",
    "                                       reuse=None)\n",
    "\n",
    "# Compile Deep Network\n",
    "autoencoder_layers = tf.nn.rnn_cell.MultiRNNCell([encoder_dropout, decoder_cell], \n",
    "                                             state_is_tuple=True)\n",
    "\n",
    "\n",
    "# Run Dynamic RNN\n",
    "#   encoder_outpus: [batch_size, max_time, num_units]\n",
    "#   encoder_state: [batch_size, num_units]\n",
    "outputs, state = tf.nn.dynamic_rnn(autoencoder_layers,\n",
    "                                   encoder_inputs,\n",
    "                                   sequence_length=[source_sequence_length],\n",
    "                                   time_major=_time_major,\n",
    "                                   dtype=tf.float64)\n",
    "\n",
    "# Loss Metric (unregularized)\n",
    "loss = tf.losses.mean_squared_error(labels=decoder_targets, predictions=outputs)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Optimizer\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "num_steps = 1000\n",
    "x_train = df.values[0:, :500].T.reshape((500, 1, 10))\n",
    "y_train = df.values[0:, :500].T.reshape((500, 1, 10))\n",
    "\n",
    "tic = time.time()        # Initialize time count\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        _, train_loss = sess.run([optimizer, loss], feed_dict={encoder_inputs:x_train, \n",
    "                                                               decoder_targets:y_train})\n",
    "\n",
    "        if (step % 50 == 0):\n",
    "            val_loss = sess.run([loss], feed_dict={encoder_inputs:x_train, \n",
    "                                                     decoder_targets:y_train})\n",
    "            \n",
    "            print(\"-\"*50)\n",
    "            print(\"Training step: {0} - Loss = {1}\".format(step, val_loss))\n",
    "        if (step % 1000 == 0):\n",
    "            pred = sess.run([outputs], feed_dict={encoder_inputs:x_train})\n",
    "            print(y_train)\n",
    "            print(pred)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "toc = time.time()\n",
    "print(\"Overall Training Time: {}\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_encoder_net(encoder_input, decoder_target, keep_prob_in, keep_prob_out, keep_prob_state,\n",
    "                     hidden_dim, num_tickers, source_sequence_length, l2_norm=0.01, seed=235):\n",
    "    \n",
    "    # Build Encoder cell\n",
    "    encoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim,\n",
    "                                           forget_bias=1.0,\n",
    "                                           state_is_tuple=True,\n",
    "                                           activation=tf.tanh,\n",
    "                                           reuse=None)\n",
    "    \n",
    "    # Introduce Dropout to the Encoding\n",
    "    encoder_dropout = tf.contrib.rnn.DropoutWrapper(encoder_cell,\n",
    "                                                    input_keep_prob=keep_prob_in,\n",
    "                                                    output_keep_prob=keep_prob_out,\n",
    "                                                    state_keep_prob=keep_prob_state)\n",
    "    \n",
    "    \n",
    "    # Build Simple Decoder Cell\n",
    "    decoder_cell = tf.nn.rnn_cell.LSTMCell(num_units=num_tickers,\n",
    "                                           forget_bias=1.0,\n",
    "                                           state_is_tuple=True,\n",
    "                                           activation=tf.tanh,\n",
    "                                           reuse=None)\n",
    "\n",
    "    # Compile Deep Network\n",
    "    autoencoder_layers = tf.nn.rnn_cell.MultiRNNCell([encoder_dropout, decoder_cell], \n",
    "                                                 state_is_tuple=True)\n",
    "\n",
    "\n",
    "    # Run Dynamic RNN\n",
    "    #   encoder_outpus: [batch_size, max_time, num_units]\n",
    "    #   encoder_state: [batch_size, num_units]\n",
    "    outputs, state = tf.nn.dynamic_rnn(autoencoder_layers,\n",
    "                                       encoder_inputs,\n",
    "                                       sequence_length=[source_sequence_length],\n",
    "                                       time_major=True,\n",
    "                                       dtype=tf.float64)\n",
    "\n",
    "    #fc_w = [fc_layer_0.weight, fc_layer_1.weight, fc_layer_2.weight]\n",
    "\n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #l2_loss = tf.reduce_sum([tf.norm(w) for w in fc_w])\n",
    "        \n",
    "        #label = tf.one_hot(input_y, output_size)\n",
    "        #cross_entropy_loss = tf.reduce_mean(\n",
    "        #    tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=fc_layer_2.output()),\n",
    "        #    name='cross_entropy')\n",
    "        #loss = tf.add(cross_entropy_loss, l2_norm * l2_loss, name='loss')\n",
    "        loss = tf.losses.mean_squared_error(labels=decoder_targets, predictions=outputs)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        tf.summary.scalar('auto_encode_loss', loss)\n",
    "\n",
    "    return fc_layer_2.output(), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(loss, lr=1e-3):\n",
    "    with tf.name_scope('train_step'):\n",
    "        step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(loss)\n",
    "    return step\n",
    "\n",
    "def auto_encoder_training(X_train, y_train, X_val, y_val, keep_prob=0.7,\n",
    "                l2_norm=0.01, seed=235, learning_rate=1e-2, epoch=20,\n",
    "                batch_size=245, verbose=True):\n",
    "    print(\"Building my Auto-Encoder Parameters: \")\n",
    "    print(\"l2_norm={}\".format(l2_norm))\n",
    "    print(\"seed={}\".format(seed))\n",
    "    print(\"learning_rate={}\".format(learning_rate))\n",
    "\n",
    "    # define the variables and parameter needed during training\n",
    "    with tf.name_scope('inputs'):\n",
    "        xs = tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)\n",
    "        ys = tf.placeholder(shape=[None, ], dtype=tf.int64)\n",
    "        ss = tf.placeholder(shape=[None, 5], dtype=tf.float32)\n",
    "        kp = tf.placeholder(dtype=tf.float32)\n",
    "     \n",
    "    output, loss = dn2387_NN(xs, ys, kp)\n",
    "    predictions = predict(ss)\n",
    "    iters = int(X_train.shape[0] / batch_size)\n",
    "    print('number of batches for training: {}'.format(iters))\n",
    "\n",
    "    step = train_step(loss)\n",
    "    eve = evaluate(output, ys)\n",
    "\n",
    "    iter_total = 0\n",
    "    best_acc = 0\n",
    "    cur_model_name = 'dn2387_{}'.format(int(time.time()))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        merge = tf.summary.merge_all()\n",
    "\n",
    "        writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "                              \n",
    "        for epc in range(epoch):\n",
    "            print(\"epoch {} \".format(epc + 1))\n",
    "\n",
    "            for itr in range(iters):\n",
    "                iter_total += 1\n",
    "\n",
    "                training_batch_x = X_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "                training_batch_y = y_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "                \n",
    "                #import pdb\n",
    "                #pdb.set_trace()\n",
    "                _, cur_loss = sess.run([step, loss], feed_dict={xs: training_batch_x, ys: training_batch_y, kp: keep_prob})\n",
    "\n",
    "                if iter_total % 10 == 0:\n",
    "                    # do validation\n",
    "                    valid_eve, merge_result = sess.run([eve, merge], feed_dict={xs: X_val, ys: y_val, kp:1.0})\n",
    "                    valid_acc = 100 - valid_eve * 100 / y_val.shape[0]\n",
    "                    if verbose:\n",
    "                        print('{}/{} loss: {} validation accuracy : {}%'.format(\n",
    "                            batch_size * (itr + 1),\n",
    "                            X_train.shape[0],\n",
    "                            cur_loss,\n",
    "                            valid_acc))\n",
    "\n",
    "                    # save the merge result summary\n",
    "                    writer.add_summary(merge_result, iter_total)\n",
    "\n",
    "                    # when achieve the best validation accuracy, we store the model paramters\n",
    "                    if valid_acc > best_acc:\n",
    "                        print('Best validation accuracy! iteration:{} accuracy: {}%'.format(iter_total, valid_acc))\n",
    "                        best_acc = valid_acc\n",
    "                        saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "\n",
    "    print(\"Training ends. The best valid accuracy is {}. Model named {}.\".format(best_acc, cur_model_name))\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
